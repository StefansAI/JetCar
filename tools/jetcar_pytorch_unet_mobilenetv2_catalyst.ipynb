{"cells":[{"cell_type":"markdown","metadata":{"id":"UQ_zluyo1Iqy"},"source":["# Image segmentation for JetCar project"]},{"cell_type":"markdown","metadata":{"id":"EZZ_ipbM1Tq0"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n","    Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n","    View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/github.com/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"yu3P0q3C1Yoy"},"source":["# 1. Introduction"]},{"cell_type":"markdown","metadata":{"id":"MCoZFG5G1eRa"},"source":["This notebook uses Semantic Segmentation to train a U-Net model. It utilizes the packages \"segmentation-models-pytorch\" and \"catalyst\". \n","\n","Each training and validation data pair consist of\n","\n","    a jpg image with 3 channels (RGB)\n","    a png mask with 1 channel containing the class values for each pixel\n","    \n","In addition there is a set of images in a third set to create predicted masks as feedback for segmentation mask adjustments.\n","\n","The code is based on following examples:\n","* https://github.com/qubvel/segmentation_models.pytorch\n","* https://www.tensorflow.org/tutorials/images/segmentation\n","* https://github.com/usuyama/pytorch-unet\n","* https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation\n"]},{"cell_type":"markdown","metadata":{"id":"lwwuS7Fz1z74"},"source":["# 2. Preparing the Environment\n","## 2.1. List all Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3M-Lv5Nk-Gx6"},"outputs":[],"source":["from glob import glob\n","import shutil\n","import argparse\n","import zipfile\n","import hashlib\n","import requests\n","from tqdm import tqdm\n","import IPython.display as display\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import datetime,os\n","from IPython.display import clear_output\n","from urllib.parse import urlparse\n","import zipfile\n","import helper\n","import PIL.Image\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import cv2\n"]},{"cell_type":"markdown","metadata":{"id":"rwm-yxOb-ESy"},"source":["## 2.2. Define Global Constants"]},{"cell_type":"markdown","metadata":{"id":"5ympOmmRZcfV"},"source":["Defining all constants at the top allows finding them quickly and changing them easily."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXM2G5BE2THk"},"outputs":[],"source":["# Image size that we are going to use\n","IMG_SIZE = 224 \n","# Our images are RGB (3 channels)\n","N_CHANNELS = 3\n","# Scene Parsing has 22 classes including 'nothing'= 0\n","N_CLASSES = 17\n","\n","# A batch size of 100-200 works with Colab Pro and High-Ram setting, reduce when getting out-of-memory errors \n","BATCH_SIZE = 200\n","# It normally never gets to 100 or more because of early stop\n","EPOCHS = 500\n","# 1e-3 is a got starting point, you can try to repeat same with 1e-4 or smaller\n","LEARNING_RATE = 1e-3\n","# \n","ENCODER_LEARNING_RATE = LEARNING_RATE/2\n","# Number of epochs without improvement to change learning rate\n","SCHEDULER_PATIENCE = 5\n","# Number of epochs without improvement to stop training completely\n","EARLY_STOP_PATIENCE = 10\n","\n","# O0: no optimization, O1: some, O2: more, O3:all FP16\n","FP16_OPT_LEVEL = \"O3\""]},{"cell_type":"markdown","metadata":{"id":"sbn0H00b3Y0K"},"source":["## 2.3. Mount Google Drive\n","The best way to get to large self made datasets is placing them into a google drive and access it them there."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EW1affb23jDH"},"outputs":[],"source":["GDRIVE_MOUNT = \"/content/gdrive/\"\n","GDRIVE_PATH = '/content/gdrive/My Drive/JetCar/'\n","\n","from google.colab import drive\n","drive.mount(GDRIVE_MOUNT, force_remount=True)\n","!ls "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B867nkHraXqc"},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4EP9Zs3WCJw"},"outputs":[],"source":["!ls \"gdrive/My Drive/JetCar/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2AjCTF5_ArF"},"outputs":[],"source":["!ls \"/content/JetCar/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMk0KMdw4Qlq"},"outputs":[],"source":["# Define the URLs to the data to be downloaded\n","DOWNLOAD_URLS = [\n","      GDRIVE_PATH+'JetCar_DataSet.zip']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUcT0q064pW_"},"outputs":[],"source":["# Define all path and file constants\n","DATA_PATH = \"/content/JetCar/\"\n","DATASET_PATH = DATA_PATH + \"DataSet/\"\n","IMAGE_SUBDIR = \"Img/\"\n","MASK_SUBDIR = \"Mask/\"\n","TRAINING_SUBDIR = \"Train/\"\n","VALIDATION_SUBDIR = \"Val/\"\n","RECORDING_PATH = DATA_PATH +\"DataSet/Img/Test/\"\n","PREDICTION_PATH = DATA_PATH +\"Prediction/\"\n","MODEL_WEIGHT_FILE_NAME = 'JetCar_Best_MobileNetV2_Model_Weights_Catalyst.pth'\n","PREDICTION_ZIP_FILE_NAME = 'Prediction_MobileNetV2_Catalyst.zip'\n","LOG_DIR = \"./logs/segmentation\"\n","LOG_ZIP_FILE_NAME = \"JetCar_Logs_Catalyst.zip\""]},{"cell_type":"markdown","metadata":{"id":"31_Gvrqy9myg"},"source":["## 2.4. Downloading the Dataset"]},{"cell_type":"markdown","metadata":{"id":"QfwkumeJeQe8"},"source":["Define the download functions for te datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ju3mLR2U03aK"},"outputs":[],"source":["def download(source_url, destination_path=None, overwrite=False):\n","\n","    print(\"download(%s,%s,%d)\\n\"%(source_url, destination_path, overwrite))\n","\n","    if destination_path is None:\n","        fname = source_url.split('/')[-1]\n","    else:\n","        destination_path = os.path.expanduser(destination_path)\n","        if os.path.isdir(destination_path):\n","            fname = os.path.join(destination_path, source_url.split('/')[-1])\n","        else:\n","            fname = destination_path\n","\n","    if overwrite or not os.path.exists(fname):\n","        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n","        if not os.path.exists(dirname):\n","            os.makedirs(dirname)\n","\n","        scheme = urlparse(source_url).scheme\n","        if scheme == 'http' or scheme == 'https':\n","            print('Downloading %s from %s...'%(fname, source_url))\n","            r = requests.get(source_url, stream=True)\n","            if r.status_code != 200:\n","                raise RuntimeError(\"Failed downloading url %s\"%url)\n","            total_length = r.headers.get('content-length')\n","           \n","            with open(fname, 'wb') as f:\n","                if total_length is None: # no content length header\n","                    for chunk in r.iter_content(chunk_size=1024):\n","                        if chunk: # filter out keep-alive new chunks\n","                            f.write(chunk)\n","                else:\n","                    total_length = int(total_length)\n","                    for chunk in tqdm(r.iter_content(chunk_size=1024),\n","                                      total=int(total_length / 1024. + 0.5),\n","                                      unit='KB', unit_scale=False, dynamic_ncols=True):\n","                        f.write(chunk)\n","\n","        else:\n","            print('Copying %s from %s...'%(os.path.normpath(fname), os.path.normpath(source_url)))\n","            shutil.copy(os.path.normpath(source_url), os.path.normpath(fname))\n","\n","    return fname\n","\n","def download_dataset(source_urls, destination_path, overwrite=False):\n","    if not os.path.exists(destination_path):\n","        os.mkdir(destination_path)\n","    download_dir = os.path.join(destination_path, 'downloads')\n","    if not os.path.exists(download_dir):\n","        os.mkdir(download_dir)\n","    for url in source_urls:\n","        filename = download(source_url=url, destination_path=download_dir, overwrite=overwrite)\n","        # extract\n","        with zipfile.ZipFile(filename,\"r\") as zip_ref:\n","            zip_ref.extractall(path=destination_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoLJa5q803eJ"},"outputs":[],"source":["# Create local directory for data\n","if not os.path.exists(DATA_PATH):\n","  os.makedirs(DATA_PATH)\n","  download_dataset(DOWNLOAD_URLS, DATA_PATH, overwrite=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuaMEDuM7jSy"},"outputs":[],"source":["!ls \"/content/JetCar/downloads/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afkZ24W_03ht"},"outputs":[],"source":["TRAINSET_SIZE = len(glob(DATASET_PATH + IMAGE_SUBDIR + TRAINING_SUBDIR + \"*.jpg\"))\n","print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n","\n","VALSET_SIZE = len(glob(DATASET_PATH + IMAGE_SUBDIR + VALIDATION_SUBDIR + \"*.jpg\"))\n","print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")\n","\n","PREDSET_SIZE = len(glob(RECORDING_PATH + \"*.jpg\"))\n","print(f\"The Prediction Dataset contains {PREDSET_SIZE} images.\")"]},{"cell_type":"markdown","metadata":{"id":"iadf6VYBBE_d"},"source":["# Install additional packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jH3GkvowBOPt"},"outputs":[],"source":["# Catalyst\n","!pip install catalyst==20.12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfkJ3G5rBX2K"},"outputs":[],"source":["# for pretrained segmentation models for PyTorch\n","!pip install segmentation-models-pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k50NVXBYBDmN"},"outputs":[],"source":["# for tensorboard\n","!pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5AEbxVtYZWb"},"outputs":[],"source":["# Colab supports FP16\n","!git clone https://github.com/NVIDIA/apex\n","!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n","is_fp16_used = True"]},{"cell_type":"markdown","metadata":{"id":"N90-BlegJZfs"},"source":["## Enabling GPU on Colab\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCitpQdkJNdI"},"outputs":[],"source":["if not torch.cuda.is_available():\n","  raise Exception(\"GPU not available. CPU training will be too slow.\")\n","\n","import catalyst\n","from catalyst import utils\n","\n","device = utils.get_device()\n","print(f\"device: {device}\")\n","\n","print(\"device name\", torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tLthp8flRBVg"},"outputs":[],"source":["SEED = 42\n","utils.set_global_seed(SEED)\n","utils.prepare_cudnn(deterministic=True)\n","\n","print(f\"torch: {torch.__version__}, catalyst: {catalyst.__version__}\")"]},{"cell_type":"markdown","metadata":{"id":"Qg2FqLRGBEJT"},"source":["## Prepare Dataset and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQgx1h9_NJc0"},"outputs":[],"source":["def create_one_hot(code_mask):\n","  one_hot_mask = torch.nn.functional.one_hot(code_mask, N_CLASSES).float()\n","  one_hot_mask = one_hot_mask.numpy()\n","  return transforms.functional.to_tensor(one_hot_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8A3hkVW-jfx"},"outputs":[],"source":["def create_mask(one_hot_mask):\n","  n = len(list(one_hot_mask.size()))-3\n","  #print(f'create_mask n={n}')\n","  return torch.argmax(one_hot_mask, dim=n)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-UTr03eAROb"},"outputs":[],"source":["class JetCarDatasetClass(Dataset):\n","  def __init__(self, image_dir, mask_dir):\n","    self.image_dir = image_dir\n","    self.mask_dir = mask_dir\n","    self.images = glob(os.path.join(image_dir, '*.jpg'))\n","\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    image_fname = self.images[idx];\n","    img_base_name = os.path.basename(image_fname)\n","    mask_base_name = img_base_name.replace(\"Img\",\"Mask\")\n","    mask_base_name = mask_base_name.replace(\".jpg\",\".png\")\n","    mask_fname = os.path.join(self.mask_dir, mask_base_name);\n","\n","    image = PIL.Image.open(image_fname).convert('RGB')\n","    image.load()\n","    image = transforms.functional.resize(image, (IMG_SIZE, IMG_SIZE))\n","    image = np.array(image)\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    image = transforms.functional.to_tensor(image)\n","    image = transforms.functional.normalize(image, [0.485, 0.456, 0.406][::-1], [0.229, 0.224, 0.225][::-1]) # Reverse order for BGR instead of RGB processing\n","    mask = PIL.Image.open(mask_fname)\n","    mask.load()\n","    mask = transforms.functional.pil_to_tensor(mask).squeeze().long()\n","    mask = create_one_hot(mask)\n","    return [image, mask]\n","\n","\n","train_set = JetCarDatasetClass(DATASET_PATH + IMAGE_SUBDIR + TRAINING_SUBDIR, DATASET_PATH + MASK_SUBDIR + TRAINING_SUBDIR)\n","valid_set = JetCarDatasetClass(DATASET_PATH + IMAGE_SUBDIR + VALIDATION_SUBDIR, DATASET_PATH + MASK_SUBDIR + VALIDATION_SUBDIR)\n","\n","image_datasets = {\n","  'train': train_set, 'valid': valid_set\n","}\n","\n","dataloaders = {\n","  'train': DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0),\n","  'valid': DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ku9SS6lTeQVz"},"outputs":[],"source":["class JetCarPredictionClass(Dataset):\n","  def __init__(self, image_dir):\n","     self.images = glob(os.path.join(image_dir, '*.jpg'))\n","\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    image_fname = self.images[idx];\n","    img_base_name = os.path.basename(image_fname)\n","    image = PIL.Image.open(image_fname).convert('RGB')\n","    image.load()\n","    image = transforms.functional.resize(image, (IMG_SIZE, IMG_SIZE))\n","    image = np.array(image)\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    image = transforms.functional.to_tensor(image)\n","    image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    return [image, img_base_name]\n","\n","pred_set = JetCarPredictionClass(RECORDING_PATH)\n","predloader = DataLoader(pred_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"]},{"cell_type":"markdown","metadata":{"id":"BtkJTyxGB-XB"},"source":["## Check the outputs from DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRIOwoQvBKPm"},"outputs":[],"source":["def reverse_image(inp):\n","  inp = inp.numpy().transpose((1, 2, 0))\n","  mean = np.array([0.485, 0.456, 0.406][::-1])\n","  std = np.array([0.229, 0.224, 0.225][::-1])\n","  inp = std * inp + mean\n","  inp = np.clip(inp, 0, 1)\n","  inp = (inp * 255).astype(np.uint8)\n","  return cv2.cvtColor(inp, cv2.COLOR_BGR2RGB)\n","\n","def reverse_mask(inp):\n","  inp = create_mask(inp).numpy()\n","  return inp.astype(np.uint8)\n","\n","def display(display_list):\n","  plt.figure(figsize=(15, 15))\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    img = display_list[i]\n","    if img.size(0) == 3:\n","      img = reverse_image(img)\n","    else:\n","      img = reverse_mask(img)\n","    plt.imshow(img)\n","    plt.axis('off')\n","  plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DWTi8dxKNXc"},"outputs":[],"source":["# Get a set from training data\n","display_idx = random.randint(0,TRAINSET_SIZE-1)\n","print(\"Index:%d\"%(display_idx))\n","img, mask = train_set[display_idx] \n","\n","display([img,mask])"]},{"cell_type":"markdown","metadata":{"id":"gJ65Br1oDCOX"},"source":["# Instantiate the UNet model\n","\n","- Move the model to GPU\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY0Vk2VDCAiz"},"outputs":[],"source":["import segmentation_models_pytorch as smp\n","\n","# We will use Feature Pyramid Network with pre-trained mobilenet backbone\n","model = smp.Unet(encoder_name=\"mobilenet_v2\", classes=N_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaZdFgOnGA_p"},"outputs":[],"source":["# Uncomment below to show the model\n","#model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoVYhHpbCSdY"},"outputs":[],"source":["# Uncomment below to show the model\n","#from torchsummary import summary\n","#summary(model.cpu(), input_size=(3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6jGlrCsecc1J"},"outputs":[],"source":["if os.path.exists(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME): \n","    model.load_state_dict(torch.load(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME))\n","    print('Last best model weights loaded!')"]},{"cell_type":"markdown","metadata":{"id":"H7rAEQCUEI2v"},"source":["#Model training\n","\n","We will optimize loss as the sum of IoU, Dice and BCE, specifically this function: IoU+Dice+0.8∗BCE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1eHQdw6IxVv"},"outputs":[],"source":["from catalyst.contrib.nn import DiceLoss, IoULoss\n","\n","# we have multiple criterions\n","criterion = {\n","    \"dice\": DiceLoss(),\n","    \"iou\": IoULoss(),\n","    \"bce\": nn.BCEWithLogitsLoss()\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_GWeGDIK5pz"},"outputs":[],"source":["from torch import optim\n","from catalyst.contrib.nn import RAdam, Lookahead\n","\n","learning_rate = LEARNING_RATE\n","encoder_learning_rate = ENCODER_LEARNING_RATE\n","\n","# Since we use a pre-trained encoder, we will reduce the learning rate on it.\n","layerwise_params = {\"encoder*\": dict(lr=encoder_learning_rate, weight_decay=0.00003)}\n","\n","# This function removes weight_decay for biases and applies our layerwise_params\n","model_params = utils.process_model_params(model, layerwise_params=layerwise_params)\n","\n","# Catalyst has new SOTA optimizers out of box\n","base_optimizer = RAdam(model_params, lr=learning_rate, weight_decay=0.0003)\n","optimizer = Lookahead(base_optimizer)\n","\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=SCHEDULER_PATIENCE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaGslMBSMUMd"},"outputs":[],"source":["from catalyst.dl import SupervisedRunner\n","\n","# by default SupervisedRunner uses \"features\" and \"targets\",\n","# in our case we get \"image\" and \"mask\" keys in dataset __getitem__\n","runner = SupervisedRunner(device=device, input_key=\"image\", input_target_key=\"mask\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBo0QFtF9qAi"},"outputs":[],"source":["if is_fp16_used:\n","    fp16_params = dict(opt_level=FP16_OPT_LEVEL) # params for FP16\n","else:\n","    fp16_params = None\n","\n","print(f\"FP16 params: {fp16_params}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0dpZq--Mg5D"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir {LOG_DIR}"]},{"cell_type":"markdown","metadata":{"id":"adcdAu9ZEOLG"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZD7ebr7CTe0"},"outputs":[],"source":["from catalyst.dl import EarlyStoppingCallback\n","\n","early_stop_callback = EarlyStoppingCallback(patience=EARLY_STOP_PATIENCE, metric=\"loss\", minimize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ii-pLcdNbEqG"},"outputs":[],"source":["from catalyst.dl import Callback, CallbackOrder, CallbackNode, IRunner\n","\n","class SaveBestModelCallback(Callback):\n","    def __init__(\n","        self,\n","        metric: str = \"loss\",\n","        minimize: bool = True,\n","        min_delta: float = 1e-6,\n","        filename: str = \"best_model.pth\"\n","    ):\n","        super().__init__(order=CallbackOrder.external, node=CallbackNode.all)\n","        self.best_score = None\n","        self.metric = metric\n","        self.num_bad_epochs = 0\n","        self.is_better = None\n","\n","        if minimize:\n","            self.is_better = lambda score, best: score <= (best - min_delta)\n","        else:\n","            self.is_better = lambda score, best: score >= (best + min_delta)\n","\n","    def on_epoch_end(self, runner: \"IRunner\") -> None:\n","        if runner.stage.startswith(\"infer\"):\n","            return\n","\n","        score = runner.valid_metrics[self.metric]\n","        if self.best_score is None or self.is_better(score, self.best_score):\n","            print(f'Loss improved from {self.best_score} to {score}. ->Saving model weights!')\n","            torch.save(model.state_dict(), GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME)\n","            self.num_bad_epochs = 0\n","            self.best_score = score\n","        else:\n","            self.num_bad_epochs += 1\n","            print(f'Loss did not improve from {self.best_score} for {self.num_bad_epochs} epoch(s)')\n"," \n","\n","save_best_model_callback = SaveBestModelCallback(filename=GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RfxgL303EMiy"},"outputs":[],"source":["from catalyst.dl import DiceCallback, IouCallback, \\\n","  CriterionCallback, MetricAggregationCallback\n","\n","callbacks = [\n","    # Each criterion is calculated separately.\n","    CriterionCallback(\n","        input_key=\"mask\",\n","        prefix=\"loss_dice\",\n","        criterion_key=\"dice\"\n","    ),\n","    CriterionCallback(\n","        input_key=\"mask\",\n","        prefix=\"loss_iou\",\n","        criterion_key=\"iou\"\n","    ),\n","    CriterionCallback(\n","        input_key=\"mask\",\n","        prefix=\"loss_bce\",\n","        criterion_key=\"bce\"\n","    ),\n","\n","    # And only then we aggregate everything into one loss.\n","    MetricAggregationCallback(\n","        prefix=\"loss\",\n","        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n","        # because we want weighted sum, we need to add scale for each loss\n","        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n","    ),\n","\n","    # metrics\n","    DiceCallback(input_key=\"mask\"),\n","    IouCallback(input_key=\"mask\"),\n","    \n","    early_stop_callback,\n","    save_best_model_callback\n","]\n","\n","runner.train(\n","    model=model,\n","    criterion=criterion,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    # our dataloaders\n","    loaders=dataloaders,\n","    # We can specify the callbacks list for the experiment;\n","    callbacks=callbacks,\n","    # path to save logs\n","    logdir=LOG_DIR,\n","    num_epochs=EPOCHS,\n","    # save our best checkpoint by IoU metric\n","    main_metric=\"iou\",\n","    # IoU needs to be maximized.\n","    minimize_metric=False,\n","    # for FP16. It uses the variable from the very first cell\n","    fp16=fp16_params,\n","    # prints train logs\n","    verbose=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_hnwXSgTu62p"},"outputs":[],"source":["print('Done!')"]},{"cell_type":"markdown","metadata":{"id":"xR2WQqPm5vcd"},"source":["At this point the training is done, very likely terminated via early stopping. Now, the logs can be copied to the google drive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TKJ84l0qU-ca"},"outputs":[],"source":["#shutil.copy(os.path.normpath(checkpoint_path), os.path.normpath(GDRIVE_PATH+checkpoint_path))\n","#torch.save(model.state_dict(), GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T8t7bbj4Zqcf"},"outputs":[],"source":["# Function to zip up a complete directory and add all files to the zip file.\n","def zipdir(dir_to_archive, archive_filename):\n","    ziph = zipfile.ZipFile(archive_filename, 'w', zipfile.ZIP_DEFLATED)\n","    for root, dirs, files in os.walk(dir_to_archive):\n","        for file in files:\n","            if file != archive_filename:\n","                ziph.write(os.path.join(root, file))\n","    ziph.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgWrTMf_DJSK"},"outputs":[],"source":["# Zip log directory and copy to google drive\n","if os.path.exists(LOG_DIR): \n","    print(\"Compressing logs\")\n","    zipdir(LOG_DIR, DATA_PATH + LOG_ZIP_FILE_NAME)\n","    print('Copying %s to %s...'%(LOG_ZIP_FILE_NAME, GDRIVE_PATH))\n","    shutil.copy(os.path.normpath(DATA_PATH + LOG_ZIP_FILE_NAME), \n","                os.path.normpath(GDRIVE_PATH + LOG_ZIP_FILE_NAME))"]},{"cell_type":"markdown","metadata":{"id":"lcRgjfk5D-kP"},"source":["## Predict new images using the trained model\n","Besides training and validation sets, there is a third set to test the model. This set just contains JPG images and below the model is invoked to create mask PNG files for that set. This allows visually inspecting the output of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZs5mQykFGzA"},"outputs":[],"source":["if os.path.exists(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME): \n","    model.load_state_dict(torch.load(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME))\n","    print('Last best model weights re-loaded!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZy4_YTDisAV"},"outputs":[],"source":["if not os.path.exists(PREDICTION_PATH):\n","    os.makedirs(PREDICTION_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CiN8qhPiyWl"},"outputs":[],"source":["if os.path.exists(PREDICTION_PATH + PREDICTION_ZIP_FILE_NAME):\n","    os.remove(PREDICTION_PATH + PREDICTION_ZIP_FILE_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CnGpB5yAlV8b"},"outputs":[],"source":["zipObj = zipfile.ZipFile(PREDICTION_PATH+PREDICTION_ZIP_FILE_NAME, 'w')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RT83bNd0laYe"},"outputs":[],"source":["model.eval()   # Set model to the evaluation mode\n","n=0\n","for step, (image_batch, filename_batch) in enumerate(predloader):\n","  with torch.no_grad():\n","    pred_batch = model(image_batch.to(device))\n","    mask_batch = create_mask(pred_batch).cpu()\n","    for i in range(image_batch.shape[0]):\n","        base_filename = filename_batch[i]\n","        base_filename = os.path.basename(base_filename)\n","        base_filename = os.path.splitext(base_filename)[0]\n","        pred_filename = base_filename.replace(\"Img\",\"Pred\")+\".png\"\n","        n = n+1\n","        print(f\"Saving file #{n}: \" + pred_filename)\n","        torchvision.utils.save_image(mask_batch[i]/255.0, PREDICTION_PATH + pred_filename, normalize=False, scale_each=False)\n","        zipObj.write(PREDICTION_PATH + pred_filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jSDl_DJxSi-P"},"outputs":[],"source":["zipObj.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0s-Q6yENYI9I"},"outputs":[],"source":["print('Copying %s to %s...'%(PREDICTION_ZIP_FILE_NAME, GDRIVE_PATH))\n","shutil.copy(os.path.normpath(PREDICTION_PATH+PREDICTION_ZIP_FILE_NAME), \n","                 os.path.normpath(GDRIVE_PATH+PREDICTION_ZIP_FILE_NAME))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tavixmeXZxu7"},"outputs":[],"source":["print(\"All Done!\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","toc_visible":true,"provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}